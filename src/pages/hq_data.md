---
title: "HQ Data"
date: "2020-07-27"
tags: ["rct_design"]
---

We want data from all the respondents in the sample pool. Attrition could cause for imbalances and biases. Of the data collected, is it an accurate representation of our respondents?

Tracking respondents:

Once we've defined our units of analysis and clusters, we can allocate teams with specified schedules and tasks, equipping them with tracking sheets or pre-loaded data. We also take whatever information is coming in from the field and compile it as necessary, bringing our data together onto a shared database. We continuously assess what is missing and what can be done about it, as well as surveyors' productivity levels. As we do so, we update our timelines accordingly as this directly ties into our budgets.

We take care of allocation after extracting any relevant information about our respondents. We assign respondents to clusters as well as the dates they'll be surveying. When we do begin allocating, we want to double-blind, meaning our surveyors don't know the treatment status of who they're surveying.

Tracking sheets are typically the first page of the questionnaire, and will be removed. Therefore we want to have the unique ID's listed on every sheet of the questionnaire so asm to prevent data loss.

Once a questionnaire is completed on the field, that information is consolidated with a higher-level tracking sheet, back in the office. The data is then cross-checked so as to make sure things are lined up and data isn't missing or contradictory.

Digital Surveys:

GPS can help navigate. Digital tracking can help prevent errors involved with manual entry. This can be done in a number of ways, such as helping narrow down choices by filtering categories, scanning QR codes, and selecting cases from a list.

Obviously, digital saves paper.

We can continuously wirelessly upload shit. We can encrypt and decrypt data. We can compare across datasets easily.

Reconciliation & Consolidation:

A surveyor might say they got the job done, but the data isn't there. Or vice versa.

We want to look at response rates and update our productivity estimates.

When people are missing, we take note and try to track them down later. At time, respondents are unwilling to commit to answering questions. If they're unwilling, that's nonconsent (according to the IRB). Other times, something might've simply come up or a respondent was unable to answer for whatever reason (didn't have the time and the like).

Collecting Accurate Data:

Enumerators have difficult, painstaking jobs. They can be tempted to fake shit, or run into the wrong person, or not want to expend additional effort because it's just so much to deal with. Maybe enumerators think they just know the "right" answer. They don't share the same perspective as an evaluator, who has a higher level take on things. It's possible enumerators could be overwhelmed and not want to "unlock" additional, strenuous questions. It's our job to understand these temptations and negative motivations.

Spot checks, audits, accompaniments, back-checks, and audio audits are all ways of ensuring things are going according to plan.

High frequency checks (HFC) can use a broader perspective that encompasses data across questionnaires, to ask different questions or get a different take. HFC's check for validity of data, evaluating things such as the survey instrument itself and statistical distributions.

When back-checking, we want to test for fraud by asking questions that are unlikely to change and are straightforward, to detect for lying or fraud on the enumerator's part. We also want to test for questions that an enumerator may find problematic, for example because it'd cause the survey to drag on.

If a survey is complicated, it's worth back-checking the questions that make it so, since there's a higher risk of mistakes or oversight even from honest enumerators. We also want to see if things were answered properly depending on the format of answer we're looking for.

HFC's can be validated digitally with basic logic checks as well as to detect patterns of fraud per enumerator. We can check for indicators of low-effort or "filling out the interview underneath a tree," such as how many questions are answered "refusal," or observing the typical distribution of responses when dealing with an enumerator, average interview time, etc. Perhaps the enumerator's interview style is influencing outcomes.

We can also compare data as compared to our expectations.
